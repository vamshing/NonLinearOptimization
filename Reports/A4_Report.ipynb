{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR 506: HomeWork-4\n",
    "**Created in iPython Notebook by Guduguntla Vamshi <gudugu@ncsu.edu>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.Conjugate Gradient Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\begin{equation}\n",
    "min \\ f(x),\\ f(x) = 0.5x^T A x - b^T x\n",
    "\\\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "\\ A = \\begin{pmatrix}\n",
    "4 & 0 & 0 & 1 & 0 & 0\\\\\n",
    "4 & 4 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 5 & 0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 5 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 6 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 6 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\begin{equation}\n",
    "\\ Using \\ b^T = [4,-8,16,1,-2,9]\n",
    "\\\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing the vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.matrix([[0],[0],[0],[0],[0],[0]])\n",
    "A  = np.matrix([[4,0,0,1,0,0],[0,4,0,0,1,0],[0,0,5,0,0,1],\n",
    "                [1,0,0,5,0,0],[0,1,0,0,6,0],[0,0,1,0,0,6]]) \n",
    "b =  np.matrix([[4],[-8],[16],[1],[-2],[9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Function to compute the minimizer vector by Conjugate gradient method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0, tol = 1.0e-8, max_iter = 100):\n",
    "    \"\"\"\n",
    "    A function to solve [A]{x} = {b} linear equation system with the \n",
    "    conjugate gradient method.\n",
    "    \n",
    "    :param A : array \n",
    "        A real symmetric positive definite matrix(assumed)\n",
    "        \n",
    "    :param b : vector\n",
    "        The vector of the system which is given in RHS.\n",
    "        \n",
    "    :param x0 : vector\n",
    "        The starting guess for the solution.\n",
    "        \n",
    "    :param max_iter : integer\n",
    "        Maximum number of iterations. Iteration will stop after \n",
    "        max_iter steps even if the specified tolerance \n",
    "        has not been achieved.\n",
    "        \n",
    "    :param tol : float\n",
    "        Tolerance to achieve. The algorithm will terminate when either \n",
    "        the relative or the absolute residual is below tol.\n",
    "        \n",
    "    :var    r0 : vector\n",
    "                 Initialization stores the value (b - a * A )\n",
    "    \n",
    "    :var    d  : vector\n",
    "    \n",
    "    :var    a  : float\n",
    "                 Iteratively computes the \n",
    "                 scalar of (r1T.r1)/(r0T.r0)\n",
    "\n",
    "    :var    ri : vector\n",
    "                 Iteratively stores the value (r - a * A * d), \n",
    "                 used to check for the convergence\n",
    "    \n",
    "    :var    x  : vector \n",
    "                 Stores the solution for the next iteration iteratively\n",
    "                 \n",
    "    :var    b  : float\n",
    "                 Iteratively computes the scalar of (riT.ri)/(diT.A.di)\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    r0 = b - np.dot(A, x)\n",
    "    d = r0\n",
    "\n",
    "#   Iterations:   \n",
    "    for i in xrange(max_iter):\n",
    "        a = float(np.dot(r0.T, r0)/np.dot(np.dot(d.T, A), d))\n",
    "        x = x + d*a\n",
    "        ri = r0 - np.dot(A*a, d)\n",
    "        \n",
    "        print \"iteration: \",i, \"r(i): \",round(np.linalg.norm(ri),5)\n",
    "\n",
    "        if np.linalg.norm(ri) < tol:\n",
    "            print \"\\nConverged Successfully in iterations :\",i\n",
    "            print \"The result of vector x:\"\n",
    "            return np.around(x,decimals=10)\n",
    "            break\n",
    "        b = float(np.dot(ri.T, ri)/np.dot(r0.T, r0))\n",
    "        d = ri + b * d\n",
    "        r0 = ri\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 r(i):  20.54264\n",
      "iteration:  1 r(i):  10.48844\n",
      "iteration:  2 r(i):  5.69484\n",
      "iteration:  3 r(i):  3.39053\n",
      "iteration:  4 r(i):  2.18629\n",
      "iteration:  5 r(i):  1.41935\n",
      "iteration:  6 r(i):  0.86272\n",
      "iteration:  7 r(i):  0.48876\n",
      "iteration:  8 r(i):  0.27119\n",
      "iteration:  9 r(i):  0.15636\n",
      "iteration:  10 r(i):  0.09683\n",
      "iteration:  11 r(i):  0.06273\n",
      "iteration:  12 r(i):  0.03945\n",
      "iteration:  13 r(i):  0.02287\n",
      "iteration:  14 r(i):  0.01234\n",
      "iteration:  15 r(i):  0.00647\n",
      "iteration:  16 r(i):  0.00345\n",
      "iteration:  17 r(i):  0.00197\n",
      "iteration:  18 r(i):  0.00123\n",
      "iteration:  19 r(i):  0.00083\n",
      "iteration:  20 r(i):  0.00055\n",
      "iteration:  21 r(i):  0.00034\n",
      "iteration:  22 r(i):  0.0002\n",
      "iteration:  23 r(i):  0.00011\n",
      "iteration:  24 r(i):  6e-05\n",
      "iteration:  25 r(i):  4e-05\n",
      "iteration:  26 r(i):  2e-05\n",
      "iteration:  27 r(i):  1e-05\n",
      "iteration:  28 r(i):  1e-05\n",
      "iteration:  29 r(i):  0.0\n",
      "iteration:  30 r(i):  0.0\n",
      "iteration:  31 r(i):  0.0\n",
      "iteration:  32 r(i):  0.0\n",
      "iteration:  33 r(i):  0.0\n",
      "iteration:  34 r(i):  0.0\n",
      "iteration:  35 r(i):  0.0\n",
      "iteration:  36 r(i):  0.0\n",
      "iteration:  37 r(i):  0.0\n",
      "iteration:  38 r(i):  0.0\n",
      "iteration:  39 r(i):  0.0\n",
      "iteration:  40 r(i):  0.0\n",
      "iteration:  41 r(i):  0.0\n",
      "iteration:  42 r(i):  0.0\n",
      "iteration:  43 r(i):  0.0\n",
      "iteration:  44 r(i):  0.0\n",
      "iteration:  45 r(i):  0.0\n",
      "iteration:  46 r(i):  0.0\n",
      "iteration:  47 r(i):  0.0\n",
      "iteration:  48 r(i):  0.0\n",
      "iteration:  49 r(i):  0.0\n",
      "\n",
      "Converged Successfully in iterations : 49\n",
      "The result of vector x:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-2.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [-0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conjugate_gradient(A, b, x0, tol = 1.0e-10, \n",
    "                   max_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments on Results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convergence criteria is: 100 maximum iterations or **||ri|| ≤ 10^(-8)**\n",
    "- Using the initial guess as xT = [0,0,0,0,0,0] approxiamated value of x:\n",
    "  - is **[1.0,-2.0,3.0,0,0,1.0]**\n",
    "- The algorithm converged in **49 iterations** using the above guess vector\n",
    "- The computed values are cross-validated:\n",
    "   - by plugging in these values given by the criteria defined above \n",
    "   - and by checking for convergence ~0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on Method Used:**\n",
    "- The conjugate gradient method works by generating a set of vectors d :\n",
    "   - that are conjugate with respect to the matrix A. \n",
    "   - that is, **dTi A dj = 0, i != j**\n",
    "- The formula for αi( used as **a** here) corresponds to an:\n",
    "  - **exact line search along the direction di(used as d in     the code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.Non-Linear Conjugate Gradient Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Minimize***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$min \\ f(x),\\ f(x) = \\sum\\limits_{i=1}^{n-1} (x_i - 2x^2_{i+1})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ x_0 = [1,1,1,1]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing the vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function Defition\n",
    "\n",
    "def func(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return f.subs({x:a,y:b,z:c,w:d})\n",
    "\n",
    "def derv_x(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,x).subs({x:a,y:b,z:c,w:d})\n",
    "    \n",
    "def derv_y(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,y).subs({x:a,y:b,z:c,w:d})    \n",
    "    \n",
    "def derv_z(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,z).subs({x:a,y:b,z:c,w:d})\n",
    "    \n",
    "def derv_w(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,w).subs({x:a,y:b,z:c,w:d})\n",
    "\n",
    "def grad_vector(x0):\n",
    "    return np.matrix([derv_x(x0[0],x0[1],x0[2],x0[3]),\n",
    "                      derv_y(x0[0],x0[1],x0[2],x0[3]),\n",
    "                      derv_z(x0[0],x0[1],x0[2],x0[3]),\n",
    "                      derv_w(x0[0],x0[1],x0[2],x0[3])])\n",
    "def grad_vector1(x0):\n",
    "    return np.array([derv_x(x0[0],x0[1],x0[2],x0[3]),\n",
    "                     derv_y(x0[0],x0[1],x0[2],x0[3]),\n",
    "                     derv_z(x0[0],x0[1],x0[2],x0[3]),\n",
    "                     derv_w(x0[0],x0[1],x0[2],x0[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-2, 6, 6, 8]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_vector(x0) # gradient vector at x0 - guess vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def non_linear_conjugate_gradient(x0, tol = 1.0e-8, max_iter = 4):\n",
    "    \"\"\"\n",
    "    A function to solve [A]{x} = {b} linear equation system with the \n",
    "    conjugate gradient method.\n",
    "        \n",
    "    :param x0 : vector\n",
    "        The starting guess for the solution.\n",
    "        \n",
    "    :param max_iter : integer\n",
    "        Maximum number of iterations. Iteration will stop after max_iter \n",
    "        steps even if the specified tolerance has not been achieved.\n",
    "        \n",
    "    :param tol : float\n",
    "        Tolerance to achieve. The algorithm will terminate when either \n",
    "        the relative or the absolute residual is below tol.\n",
    "        \n",
    "    :var    r0 : vector\n",
    "                 Initialization stores the gradient of x0 * (-1)\n",
    "    \n",
    "    :var    d  : vector\n",
    "    \n",
    "    :var    a  : float\n",
    "                 Iteratively computes the vector of x\n",
    "\n",
    "    :var    ri : vector\n",
    "                 Iteratively stores the value norm of x ( or a), \n",
    "                 used to check for the convergence\n",
    "    \n",
    "    :var    x  : vector \n",
    "                 Stores the solution for the next iteration \n",
    "                 iteratively\n",
    "    \n",
    "    :sym    lmda  : symbol \n",
    "                 Used to compute the lamda value using \n",
    "                 Goldstein Armijo criteria\n",
    "                 \n",
    "    \"\"\"\n",
    "    x = np.matrix(x0)\n",
    "    r0 = grad_vector(x0) * (-1)\n",
    "    d = r0\n",
    "    alpha = 0.5\n",
    "\n",
    "#   Iterations:   \n",
    "    for i in xrange(max_iter):\n",
    "        \n",
    "        #compute the lamda value by line search\n",
    "        lmda = Symbol('lmda',real = True)\n",
    "        # function value at x0\n",
    "        f_x0 = func(x0[0],x0[1],x0[2],x0[3])\n",
    "        # The Goldstien-Armijo criteria for the lambda selection\n",
    "        rhs = f_x0 + np.dot(grad_vector(x0),d.T)*(alpha)*(lmda)\n",
    "        lhs = func(x0[0] + lmda*d.item(0),x0[1] + \n",
    "                   lmda*d.item(1),x0[2] + lmda*d.item(2),x0[3] + \n",
    "                   lmda*d.item(3))\n",
    "        # solver for the lamda value from quadratic inequality\n",
    "        try:\n",
    "            lmda_value = max(solve(lhs-rhs,lmda))\n",
    "        except ValueError: \n",
    "            pass\n",
    "        \n",
    "        # line search method ends there\n",
    "        x = x + np.multiply(d,lmda_value)\n",
    "        a = np.array([x.item(0),x.item(1),x.item(2),x.item(3)])\n",
    "        print \"iteration: \",i, \"r(i): \",\n",
    "        round(math.sqrt(grad_vector1(a)[0]**2 + grad_vector1(a)[1]**2 + \n",
    "              grad_vector1(a)[2]**2  + grad_vector1(a)[3]**2),5)\n",
    "        if math.sqrt(x.item(0)**2 + x.item(0)**2 + \n",
    "                      x.item(0)**2  + x.item(0)**2) < tol:\n",
    "            print \"\\nConverged Successfully in iterations :\",i\n",
    "            print \"The result of vector x:\"\n",
    "            return x\n",
    "            break\n",
    "        b = float((grad_vector(a)*grad_vector(a).T)/(\n",
    "                  grad_vector(x0)*grad_vector(x0).T))\n",
    "        d = grad_vector(a) * (-1) + d * b\n",
    "        x0 = a\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 Iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 r(i):  2.27121\n",
      "iteration:  1 r(i):  0.8224\n",
      "iteration:  2 r(i):  0.44142\n",
      "iteration:  3 r(i):  0.18847\n",
      "iteration:  4 r(i):  0.09744\n",
      "iteration:  5 r(i):  0.0449\n",
      "iteration:  6 r(i):  0.02238\n",
      "iteration:  7 r(i):  0.01097\n",
      "iteration:  8 r(i):  0.00506\n",
      "iteration:  9 r(i):  0.00261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1.10646976973941, 0.743843588792184, 0.610066169803682,\n",
       "         0.552504690335135]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_conjugate_gradient(x0, tol = 1.0e-8, max_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 r(i):  2.27121\n",
      "iteration:  1 r(i):  0.8224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1.08908941845879, 0.764107786237180, 0.667004995370713,\n",
       "         0.631226288110453]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_conjugate_gradient(x0, tol = 1.0e-8, max_iter = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments on Results**:\n",
    "- No. of Iterations :** 10 iterations**\n",
    "- Using the initial guess as **[1,1,1,1]** approxiamated value of x is **[1.106, 0.744, 0.61,0.55]**\n",
    "\n",
    "- After **2 iterations**, the value of **r(i) = 0.8224**,value of x is **[1.089, 0.764, 0.667,0.631]**\n",
    "\n",
    "\n",
    "**Comments on Method**:\n",
    "- The evaluation of the non - linear function is done using n = 4.\n",
    "- The alpha value is chosen to be **0.5**\n",
    "- The lambda value is calculated using the maximum of the two roots from the quadratic inequality:\n",
    "  - of the Goldstein - Armijo criteria.\n",
    "- Please note that the lambda value can take any value between the roots, due to the inequality( lambda term to the L.H.S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
