{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Non-Linear Conjugate Gradient Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Minimize***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$min \\ f(x),\\ f(x) = \\sum\\limits_{i=1}^{n-1} (x_i - 2x^2_{i+1})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ x_0 = [1,1,1,1]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing the vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function Defition\n",
    "\n",
    "def func(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return f.subs({x:a,y:b,z:c,w:d})\n",
    "\n",
    "def derv_x(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,x).subs({x:a,y:b,z:c,w:d})\n",
    "    \n",
    "def derv_y(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,y).subs({x:a,y:b,z:c,w:d})    \n",
    "    \n",
    "def derv_z(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,z).subs({x:a,y:b,z:c,w:d})\n",
    "    \n",
    "def derv_w(a,b,c,d):\n",
    "    x,y,z,w=symbols('x y z w')\n",
    "    f = (x - 2*y**2)**2 + (y - 2*z**2)**2 + (z - 2*w**2)**2\n",
    "    return diff(f,w).subs({x:a,y:b,z:c,w:d})\n",
    "\n",
    "def grad_vector(x0):\n",
    "    return np.matrix([derv_x(x0[0],x0[1],x0[2],x0[3]),derv_y(x0[0],x0[1],x0[2],x0[3]),\n",
    "                         derv_z(x0[0],x0[1],x0[2],x0[3]),derv_w(x0[0],x0[1],x0[2],x0[3])])\n",
    "def grad_vector1(x0):\n",
    "    return np.array([derv_x(x0[0],x0[1],x0[2],x0[3]),derv_y(x0[0],x0[1],x0[2],x0[3]),\n",
    "                         derv_z(x0[0],x0[1],x0[2],x0[3]),derv_w(x0[0],x0[1],x0[2],x0[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-2, 6, 6, 8]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_vector(x0) # gradient vector at x0 - guess vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def non_linear_conjugate_gradient(x0, tol = 1.0e-8, max_iter):\n",
    "    \"\"\"\n",
    "    A function to solve [A]{x} = {b} linear equation system with the \n",
    "    conjugate gradient method.\n",
    "        \n",
    "    :param x0 : vector\n",
    "        The starting guess for the solution.\n",
    "        \n",
    "    :param max_iter : integer\n",
    "        Maximum number of iterations. Iteration will stop after max_iter \n",
    "        steps even if the specified tolerance has not been achieved.\n",
    "        \n",
    "    :param tol : float\n",
    "        Tolerance to achieve. The algorithm will terminate when either \n",
    "        the relative or the absolute residual is below tol.\n",
    "        \n",
    "    :var    r0 : vector\n",
    "                 Initialization stores the gradient of x0 * (-1)\n",
    "    \n",
    "    :var    d  : vector\n",
    "    \n",
    "    :var    a  : float\n",
    "                 Iteratively computes the vector of x\n",
    "\n",
    "    :var    ri : vector\n",
    "                 Iteratively stores the value norm of x ( or a), used to check for the convergence\n",
    "    \n",
    "    :var    x  : vector \n",
    "                 Stores the solution for the next iteration iteratively\n",
    "    \n",
    "    :sym    lmda  : symbol \n",
    "                 Used to compute the lamda value using Goldstein Armijo criteria\n",
    "                 \n",
    "    \"\"\"\n",
    "    x = np.matrix(x0)\n",
    "    r0 = grad_vector(x0) * (-1)\n",
    "    d = r0\n",
    "    alpha = 0.5\n",
    "\n",
    "#   Iterations:   \n",
    "    for i in xrange(max_iter):\n",
    "        \n",
    "        #compute the lamda value by line search\n",
    "        lmda = Symbol('lmda',real = True)\n",
    "        # function value at x0\n",
    "        f_x0 = func(x0[0],x0[1],x0[2],x0[3])\n",
    "        # The Goldstien-Armijo criteria for the lambda selection\n",
    "        rhs = f_x0 + np.dot(grad_vector(x0),d.T)*(alpha)*(lmda)\n",
    "        lhs = func(x0[0] + lmda*d.item(0),x0[1] + lmda*d.item(1),x0[2] + lmda*d.item(2),x0[3] + lmda*d.item(3))\n",
    "        # solver for the lamda value from quadratic inequality\n",
    "        try:\n",
    "            lmda_value = max(solve(lhs-rhs,lmda))\n",
    "        except ValueError: \n",
    "            pass\n",
    "        \n",
    "        # line search method ends there\n",
    "        x = x + np.multiply(d,lmda_value)\n",
    "        a = np.array([x.item(0),x.item(1),x.item(2),x.item(3)])\n",
    "        print \"iteration: \",i, \"r(i): \",round(math.sqrt(grad_vector1(a)[0]**2 + grad_vector1(a)[1]**2 + grad_vector1(a)[2]**2  + grad_vector1(a)[3]**2),5)\n",
    "        if math.sqrt(x.item(0)**2 + x.item(0)**2 + x.item(0)**2  + x.item(0)**2) < tol:\n",
    "            print \"\\nConverged Successfully in iterations :\",i\n",
    "            print \"The result of vector x:\"\n",
    "            return x\n",
    "            break\n",
    "        b = float((grad_vector(a)*grad_vector(a).T)/(grad_vector(x0)*grad_vector(x0).T))\n",
    "        d = grad_vector(a) * (-1) + d * b\n",
    "        x0 = a\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 Iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 r(i):  2.27121\n",
      "iteration:  1 r(i):  0.8224\n",
      "iteration:  2 r(i):  0.44142\n",
      "iteration:  3 r(i):  0.18847\n",
      "iteration:  4 r(i):  0.09744\n",
      "iteration:  5 r(i):  0.0449\n",
      "iteration:  6 r(i):  0.02238\n",
      "iteration:  7 r(i):  0.01097\n",
      "iteration:  8 r(i):  0.00506\n",
      "iteration:  9 r(i):  0.00261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1.10646976973941, 0.743843588792184, 0.610066169803682,\n",
       "         0.552504690335135]], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_conjugate_gradient(x0, tol = 1.0e-8, max_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 r(i):  2.27121\n",
      "iteration:  1 r(i):  0.8224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1.08908941845879, 0.764107786237180, 0.667004995370713,\n",
       "         0.631226288110453]], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_conjugate_gradient(x0, tol = 1.0e-8, max_iter = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Comments on Results**:\n",
    "- No. of Iterations :** 10 iterations**\n",
    "- Using the initial guess as **[1,1,1,1]** approxiamated value of x is **[1.106, 0.744, 0.61,0.55]**\n",
    "\n",
    "- After **2 iterations**, the value of **r(i) = 0.8224**,value of x is **[1.089, 0.764, 0.667,0.631]**\n",
    "\n",
    "\n",
    "**Comments on Method**:\n",
    "- The evaluation of the non - linear function is done using n = 4.\n",
    "- The alpha value is chosen to be **0.5**\n",
    "- The lambda value is calculated using the maximum of the two roots from the quadratic inequality of the Goldstein - Armijo criteria.\n",
    "- Please note that the lambda value can take any value between the roots, due to the inequality( lambda term to the L.H.S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
